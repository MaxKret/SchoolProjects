# -*- coding: utf-8 -*-
"""Temporal Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kZD1LVGzyrs5KYlRvey_0j6s7muA_7W5

# Temporal analysis

## Group: &lt; Lab Group Number here &gt;

### Members: &lt; Members of the group here &gt;

We encounter time series data in pretty much every domain, from finance to weather, from public health to renewable energies. Visualizations of temporal data may represent recorded observations from the past and/or predicted developments for the future, which is why visual representations of temporal data are so important and interesting. Especially, in the context of the ongoing climate and corona crises we encounter many time series visualizations. 

This tutorial is structured into three basic phases: 1. Prepare, 2. Process, and 3. Present.

## üõí 1. Prepare 

Before we are able to do anything, we need to include the libraries that we are working with (as always):
"""

import pandas as pd
import altair as alt
import scipy.signal # for the LOESS fitting

"""### Parse dates and times

In its most basic form, time series data contain a quantitative measure that changes over time. To reference a time point we use [Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html) of Pandas as the data type for temporal entities.

With **`to_datetime()`** you can create a Timestamp with a string containing a date and/or time. Pandas can infer the date and time from various date/time strings. Let's start with the present:

"""

pd.to_datetime("now")

"""We can pass a range of date formats and Pandas will guess which numbers refer to years, months, days, hours, etc.:"""

pd.to_datetime('2020-05-07 3pm')

"""When expressing dates and times in written language, there is an ambiguity between the order of different entities. The most frequent ambiguity concerns the order of days and months, as they are typyically both expressed in double-digit numbers, unlike years that tend to be expressed with four digits. However, date conventions vary across the world.  For example, the following date might be interpreted differently depending on the country; it may refer to Saint Nicholas Day in 1929 or Anne Frank's birthday:"""

pd.to_datetime('12.6.1929')

"""To clarify towards Pandas that the first number refers to the day, you can add the parameter **`dayfirst`**:"""

pd.to_datetime('12.6.1929', dayfirst=True)

"""The method `to_datetime()` can also handle an array of date strings; it will return a `DatetimeIndex`, which is crucial for temporal indexing with Pandas."""

sessions=["2.4.2020", "9.4.2020", "16.4.2020", "23.4.2020", "7.5.2020", "14.5.2020", "28.5.2020", "4.6.2020", "11.6.2020", "25.6.2020", "2.7.2020", "9.7.2020"]
pd.to_datetime(sessions, dayfirst=True)

"""If you want to make extra sure that the date/time string is parsed correctly and quickly, you can pass a fixed **`format`** for the date/time strings to be parsed:"""

pd.to_datetime('2020-05-07', format="%Y-%m-%d")

"""*see [list of format codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes)!*

### Load time series data

In this tutorial we will be analyzing energy use in Germany from 2015 until 2018. 

First we will load the freely accessible data from the [OPSD project](https://open-power-system-data.org). `read_csv()` has a convenient feature, which lets you specify the column containing date/time information.
"""

# data downloaded from OPSD - see filter elements on this page:
# https://data.open-power-system-data.org/time_series/2019-06-05

df = pd.read_csv('https://gist.githubusercontent.com/isha211/4f5c03158c3c821ee2e428e84165cb0c/raw/ea34c6fb23aded0cd4659c364435e594305173ce/german_energy_data.csv', parse_dates=['datetime']) # parse timestamp column

"""‚úèÔ∏è *Check out the structure and content of `opsd` ‚Äî Hint: `info()` and `head()` are your friends, and `tail()` which gives the end of the list*"""

df.head()

"""
The time parsing resulted in one hour to be included from 2014. This is because this dataset covers the years 2015-2018 according to German time, while the generic UTC time is one hour 'behind'. To avoid having this stray hour in the old year, we revert the timestamps back to Germany's timezone:"""

df["datetime"] = df["datetime"].dt.tz_convert("Europe/Berlin")
df.head()

"""Did you notice that solar energy appears to be zero at the beginning and at the end of the DataFrame? Take a look at the time and you'll know why.

In order to get a random data `sample()` there is a method of the same name, which gives us a random set of rows:
"""

df.sample(10)

"""Now, let's take a look at the data types and values contained in our DataFrame:"""

df.info()

"""The values in the columns `load`, `solar` and `wind` are provided in the unit megawatt (MW) as integers.

While we did already parse the `datetime` column into the respective datetime type, it currently is just a regular column. To enable quick and convenient queries and aggregations, we need to turn it into the index of the DataFrame:
"""

df = df.set_index("datetime")

"""When we run `df.info()` again, you will see that the DataFrame now has a `DatetimeIndex`:"""

df.info()

"""The `DatetimeIndex` provides a few handy methods to extract temporal units such as months, days, week of the year, etc.: """

df.index.year.unique()

"""*Look here for syntax to extract any other [temporal attributes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html)*

## üïí 2. Process

The dataset contains over 35000 rows each of which contains data on overall energy load and renewable energy production. To make sense of all this data, we need to process the time series data into various chunks and sizes!

### Query time points and spans

A particular powerful feature of the Pandas DataFrame is its indexing capability that also works using time-based entities, such as dates and times. We have already created the index above, so let's put it to use.

One useful function of a temporal index, is its querying function. We can quickly extract the rows for a given time point or period.

Let's get the data for the day with the most daylight of 2017, i.e., the summer solstice or midsummer:
"""

df.loc["2017-06-21"]

"""Above query is an example of partial-string indexing: while our `DateTime` column actually contains time information as well, you can query it quickly (!) with just the date, or even a shorter query:"""

df.loc["2017-06"]

"""You can also query a time period. Do you remember the storm Xavier that hit Germany in early October 2017? Let's retrieve the data around this time:"""

df.loc["2017-10-04":"2017-10-06"]

"""### Aggregate values along time

The data we retrieved from OPSD, comes in the granularity of hours. To better understand the data it can be useful to reduce the data resolution and consider, for example, the total energy used/produced everyday or the daily averages over the course of entire years.

The DataFrame's `resample()` [doc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html) method provides a concise and quick way of aggregating temporally indexed data along time units. Here we create a DataFrame with summed up values for each year aggregated from the original dataset:
"""

sums = df.resample("Y").sum()
sums

"""The resample operations can also be carried out one after another. For example, we might want to know how weekly energy use/production varies between the quarters:"""

# first we create the sums per week
weekly_sums = df.resample("W").sum()
# then we generate the weekly means for each quarter
quarterly_means = weekly_sums.resample("Q").mean()
# for readability we'll revert the values back to integers
quarterly_means.astype(int)

"""## ü•ó 3. Present

Enough data processing. It's time for visualization!

### Time spans

One of the first time visualizations was [*A Chart of Biography*](https://en.wikipedia.org/wiki/A_Chart_of_Biography) (1765) by Joseph Priestley. Let's create a similar visualization of the US presidencies since World War II. First we load the CSV file with `pd.read_csv()`:
"""

presidents = pd.read_csv("https://gist.githubusercontent.com/isha211/300f50a991019f32574b702e7c274fab/raw/629f396c87c264d2714a8dd8f248623c55766c0c/us_presidents.csv")

"""The start and end times of the presidencies are given as four-digit integers, i.e., years. To convey towards Pandas and Altair that the start and end columns are actually dates, we need to parse them using the `to_datetime()` method:"""

presidents['start'] = pd.to_datetime(presidents['start'], format="%Y") 
presidents['end'] = pd.to_datetime(presidents['end'], format="%Y")

"""The following chart consists of two parts: `bars` and `labels`. The former will be the main bar chart representing the time spans of the presidencies, and the latter will add the presidents' names. This way we can position the labels right next to the bars, much nicer!"""

# we add height as a parameter to make the time spans appear less clunky
bars = alt.Chart(presidents).mark_bar(height=5).encode(
    # this time we need two columns for x-position
    x='start', x2='end',
    # we sort by start dates and hide the axis, as names are added below
    y=alt.Y('name', sort='x', axis=None),
    color="party"
)

# the labels are added to the barchart using the mark_text command
labels = bars.mark_text(align='right', dx=-5).encode(text='name')

# both charts integrated into a layered chart through the magic of a plus sign
(bars + labels).properties(width=800, height=400)

"""### Overall trends

Next, we are going to return to the energy time series data that we prepared above. Remember that the original dataset has an hourly resolution resulting in too many data points to visualize at once. Altair itself handles at most 5000 data rows.

To reduce the dataset into a manageable size, we  will create daily sums with the `resample()` method of the DateTimeIndex:
"""

days = df.resample("D").sum()

"""To visualize the contents of the DataFrame with Altair, we turn the date/time information in the index into its own column `datetime` again and transform the DataFrame from [wide to long form](https://altair-viz.github.io/user_guide/data.html#long-form-vs-wide-form-data) using the `melt()` method. We pass the name of the `datetime` column to the melt method so that it is treated as an index variable. To keep it short, these two steps `reset_index()` and `melt("datetime")` are done in one go right when we create the `Chart()`, which is what we are going to the in the rest of the tutorial.

Now let's start with a scatterplot visualization of the daily data:
"""

# Break down the long form of the data so we can see that it works
days.reset_index().melt("datetime",var_name='Type')

alt.Chart(days.reset_index().melt("datetime")).mark_circle().encode(
    x='datetime',
    y='value',
    color='variable',
).properties(width=800, height=400)

"""With this our eyes can already see several patterns going on. Some are more dictinct than others. Interestingly, the load data (blue) seems to have two separate, but parallel curves‚Ä¶

‚úèÔ∏è *There is a lot of overplotting going on. Reduce the `size` and `opacity` of all dots, by passing these as parameters to `mark_circle`!*

Next, we are going to connect the dots and create a line chart form this data. So basically the same code as above, except we're now using `mark_line()` instead of `mark_circle`:
"""

alt.Chart(days.reset_index().melt("datetime")).mark_line(strokeWidth=1).encode(
    x='datetime',
    y='value',
    color='variable',
).properties(width=800, height=400)

"""This chart already shows a lot: we can see weekly patterns‚Äîthe jittery up and down‚Äîin the load (blue) and the seasonal patterns in the form of broad waves in the load and solar curves (blue and yellow). Right around the turn of the year we see a drop of energy load.

While above line chart is truthful to the local fluctuations, it makes it hard to actually grasp the up and down over the course of months and years. Let's change the sampling from days to months to examine the overall patterns in the data.
"""

months = df.resample("M").sum()

# to make the line a bit smoother, we include an interpolate parameter
alt.Chart(months.reset_index().melt("datetime")).mark_line(opacity=0.75, interpolate="basis").encode(
    x='datetime',
    y='value',
    color='variable',
).properties(width=800, height=400)

"""What do you think? The fine-grained jitter is now gone and we might have lost too much detail. In fact, first downsampling the data and then including an interpolation is maybe giving it too much of a treatment (like overusing Photoshop's blur function). For example, the load minima around the turns of the year are not visible as the overall energy load in the winter months appears larger. One the other hand, the solar curve has become an almost perfect sine wave, probably relating to the position of the sun across the year.

üí° *Play with different granularities in he `resample()` step. For example, use **W** for week or **Q** for quarter!*

One way to integrate the local and global patterns is to create a layered graph, as we have already done with the presidents' names above. This time we are combining a line chart of the days with a line chart of monthly averages.

Note how we ensure that both DataFrames fit the same data scale. As we're looking at energy values per day, we first generate the daily sums and on this basis the monthly means:
"""

days = df.resample("D").sum()
months = days.resample("M").mean()

"""Next we create the line charts and combine the two again with the **+** operator:"""

chart1 = alt.Chart(days.reset_index().melt("datetime")).mark_line(strokeWidth=1, opacity=0.25).encode(
    x='datetime',
    y='value',
    color='variable',
).properties(width=800, height=400)

chart2 = alt.Chart(months.reset_index().melt("datetime")).mark_line(interpolate="basis", opacity=1).encode(
    x='datetime',
    y='value',
    color='variable',
)

chart1 + chart2

"""With this view we already get a good sense of the overall time patterns, while still seeing some of the particular variations.

üí° *You can add the `.interactive()` directive to one of these charts to make them zoomable!*

### Rolling windows

While the `resample()` method takes a broad brush and results in a reduced dataset and a chart with smooth curves, `rolling()` offers an alternative way of smoothing out local outliers without actually reducing the resolution of the dataset.

The first parameter determines the window size, by positioning the window at the `center` values are considered in both directions of the current date/time, and `win_type` determines how the values across the window are weighted; with the `triang` option the values further away contribute less:
"""

rolling = days.rolling(60, center=True, win_type="triang").mean()

chart1 = alt.Chart(rolling.reset_index().melt("datetime")).mark_line(strokeWidth=1.5, opacity=1).encode(
    x='datetime', y='value', color='variable',
).properties(width=800, height=400)

# same as the two charts in previous code cell, except more transparent
chart2 = alt.Chart(days.reset_index().melt("datetime")).mark_line(strokeWidth=1, opacity=0.1).encode(
    x='datetime', y='value', color='variable',
)

chart3 = alt.Chart(months.reset_index().melt("datetime")).mark_line(interpolate="basis", opacity=.25).encode(
    x='datetime', y='value', color='variable',
)

chart1 + chart2 + chart3

"""Above you see the two lines for daily sums and monthly averages from the previous cell (slightly more transparent), on top of which you can see the time curve generated with a rolling window. It is quite apparent that this curve still features more pronounced dips around the end-of-year periods and elsewhere.

üí° *Play around with different window sizes and other parameters in the first line in above cell!*

### Detailed views

Some trends seem to pan out at much lower scales, which require higher levels of detail. For example, at the beginning of 2018 we see a peak in the wind energy production. In the winter of 2017/2018 Germany and most of Europe actually witnessed several storms. And so it also happened in the first days of 2018, when [Burglind (a.k.a. Eleanor)](https://en.wikipedia.org/wiki/Storm_Eleanor_(2018)) passed by. Let's take a closer look at the first week of January 2018. For this we extract a subset of the DataFrame using our tried and tested `loc[]`, which lets us select specific time spans:
"""

burglind = df.loc["2018-01-01":"2018-01-07"]

alt.Chart(burglind.reset_index().melt("datetime")).mark_line().encode(
    x='datetime',
    y='value',
    color='variable'
).properties(width=800, height=400)

"""### High-level bars

In contrast to the detailed close-up, we might also want to look at much more general patterns. Let's say we are curious about the total production of renewable energy over the four years in the dataset. 

For this we `resample` the original data from hourly data resolution into summed up values per year:
"""

years = df.resample("Y").sum()

"""Next we create a new column, which contains the summed up values of solar and wind production."""

years["renewable"] = years["solar"]+years["wind"]

"""We `drop` the other value columns:"""

renewable = years.drop(columns=["load", "solar", "wind"])

"""Extract the years and create a specific column:"""

renewable["year"] = years.index.year
renewable

"""And we remove the datetime index, and this time discard it (i.e., `drop=True`), as we have the `year` column already:"""

renewable = renewable.reset_index(drop=True)
renewable

"""Now we can create a simple barchart from this:"""

alt.Chart(renewable).mark_bar(width=30, fill="purple").encode(
    x='year:O',
    y='renewable:Q'
).properties(width=300, height=300)

"""### Cyclical patterns

Previously, we did notice a few recurring patterns in the energy load. One pattern might be due to the weekly work-and-leasure patterns. 

So let's take a closer look at the average energy `load` over the course of a week:

"""

# remove solar and wind columns
weekdays = df.drop(columns=["solar", "wind"])

# add a column for weekdays
weekdays["weekday"] = weekdays.index.weekday

"""This time we do not use `resample()` to change the data granularity, but `groupby()` to study recurring data patterns, because we are actually interested in the energy load for each *generic* weekday across the entire dataset:"""

weekdays = weekdays.groupby("weekday").mean()

alt.Chart(weekdays.reset_index()).mark_bar(width=20).encode(
    x='weekday:O',
    y='load:Q'
).properties(width=300, height=300)

"""It's quite apparent that energy load is lower during weekends.

### Interactive selection

If you want it all: A high-level overview and detailed close-ups, you might need two coordinated views:
"""

weeks = df.resample("W").sum()
days = df.resample("D").sum()

brush = alt.selection(type='interval', encodings=['x'])

upper = alt.Chart(weeks.reset_index().melt("datetime")).mark_area(interpolate="basis").encode(
    x = alt.X('datetime:T', axis=None),
    y = alt.Y('value:Q', axis=None),
    color='variable'
).properties(width=800, height=50).add_selection(brush)

lower = alt.Chart(days.reset_index().melt("datetime")).mark_line(strokeWidth=1).encode(
    x = alt.X('datetime:T', scale=alt.Scale(domain=brush)),
    y='value',
    color='variable',
).properties(width=800, height=300)

upper & lower

"""Note: The small stacked graph above the main chart is interactive, you can adjust the viewport of the line graph by dragging a time span with it.

## Your turn

Apply your understanding of the above topics and answer the following 6 questions
"""

# 1 - Look at the documentation to translate the string to datetime using the correct format for VE day
# pd.to_datetime('8.5.1945 23:01', format="")
pd.to_datetime('8.5.1945 23:01', format="%d.%m.%Y %H:%M")

# 2 - append code to `df.index` below that shows what time zone the data is in
df.index.tz

# 3 - Use date slicing/indexing to find the mean of the solar energy from 
# 21 June 2018 and the mean of the solar energy from 20 December 2018
df.loc['06-21-2018']['solar'].mean()

df.loc['12-20-2018']['solar'].mean()

# 4 - How did daily sum energy use/production change over the years? (calculate the daily sum, then the year average)
# and show the resulting dataframe
dailysums = df.resample("d").sum()
yearlymean = dailysums.resample("Y").mean()
yearlymean_total = yearlymean[['solar','wind']]

# 5 - Create a horizontal bar chart showing total renewable energy over the years 
yearlymean_total['renewable'] = yearlymean_total['solar']+yearlymean_total['wind']
yearlymean_total['year'] = yearlymean_total.index
alt.Chart(yearlymean_total).mark_bar().encode(
    x=alt.X('renewable:Q'),
    y=alt.Y('year:O')
)

# 6 - Show the wind energy for the month of Dec 2018 along with the Savitzky‚ÄìGolay filter line overlayed on top. 
# as a line chart.  Set your window for one week of data. Play around with the polynomial order until you 
# get something you like. 
Dec_18 = df['wind'].loc["2018-12-01":"2018-12-31"]
rolling = Dec_18.rolling(30, center=True, win_type="triang").mean()

Dec = alt.Chart(Dec_18.reset_index().melt("datetime")).mark_line().encode(
    x='datetime',
    y='value',
    color='variable'
).properties(width=800, height=400)

rol = alt.Chart(rolling.reset_index().melt("datetime")).mark_line(color='purple').encode(
    x='datetime',
    y='value'
)

rol + Dec

"""## Sources

Tutorials & Examples
- [‚Äã‚Äã‚Äã‚ÄãTutorial: Time Series Analysis with Pandas by Jennifer Walker](https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/)
- [Altair Interval Selection Example](https://altair-viz.github.io/gallery/interval_selection.html)

Documentation: Pandas
- [Time series / date functionality](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)
- [Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html)
- [DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html)
- [Time-aware rolling vs. resampling](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#time-aware-rolling-vs-resampling)


"""